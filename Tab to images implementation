{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70942,"databundleVersionId":10381525,"sourceType":"competition"},{"sourceId":211253469,"sourceType":"kernelVersion"},{"sourceId":211322530,"sourceType":"kernelVersion"},{"sourceId":354127,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":295405,"modelId":316013}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dataaddict71/tabular-data-transfer-learning-implementation?scriptVersionId=235866781\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"\n### Approaching the competition for the CIBMTR - Equity in post-HCT Survival Predictions, it was clear most of the attempted solutions lacked in feature variety because simple machine learning models were vastly outperforming NNs. Additionally, most top solutions for the competition involved some forms of feature engineering.So, I came up with the idea of augmenting the data using the conversion of tabular data into images. Essentially the idea boils down to the fact that NNs have a famously bad time learning and training on tabular data not only because of the usual small size of such datasets but also because of lack of homogenity of these datasets. So, instead of bruteforcing NNs to learn from tabular data, a better approach would be converting tabular data into images using statistical scaling methods. Following which, the pre trained NN on other images can draw insights from the new converted dataset of images. So I decided to implement this research paper (by Bragilovsky et al) that closely matches with my theoretical hypothesis. https://www.sciencedirect.com/science/article/abs/pii/S1568494623007664. The paper additionally suggests using knowledge distillation on tabular to images trained NNs to further cleanse the insights drawn into the data into a useful format. This is my analysis and cross application of the research paper for this dataset/problem. \n\n\n### **Methodology**\nThis notebook combines **traditional machine learning (XGBoost)** with **neural network-based feature engineering** to predict survival probabilities for patients post-HCT. The approach leverages two key innovations from the research paper:  \n1. **Tabular-to-Image Conversion**, which transforms raw features into synthetic images for CNN training.  \n2. **Iterative FCNN Generations**, a progressive learning framework where each subsequent neural network builds on prior predictions to refine feature extraction.  \n\n---\n\n#### **1. Data Preprocessingg**\n**Goal:** Prepare data for both XGBoost and neural networks while avoiding overfitting/leakage.  \n\n- **Categorical Features:**  \n  Missing values were imputed with `\"NAN\"`, and features were factorized into integer codes:  \n  \\[\n  \\text{Factorized } c \\in \\text{Categorical Features} \\rightarrow \\text{Integer codes}\n  \\]\n- **Numerical Features:**  \n  - XGBoost handles `NaN` values directly.  \n  - Neural networks require imputation (mean strategy):  \n    \\[\n    \\text{Imputed Value} = \\frac{1}{n} \\sum_{i=1}^{n} \\text{Non-NaN Values}\n    \\]\n- **Low-Variance Features:**  \n  Removed using `VarianceThreshold` with \\( \\text{threshold} = 0.01 \\):  \n  \\[\n  \\text{Variance}(X) \\leq \\text{threshold} \\implies \\text{Feature discarded}\n  \\]\n\n---\n\n#### **2. Feature Importance Calculation**\n**Goal:** Identify the most predictive features for image generation.  \n- **Pearson Correlation:**  \n  Calculated between each feature \\( X_i \\) and survival probability \\( y \\):  \n  \\[\n  \\rho_{X_i, y} = \\frac{\\text{Cov}(X_i, y)}{\\sigma_{X_i} \\sigma_{y}}\n  \\]\n- **Sorting:**  \n  Features were reordered by \\( |\\rho_{X_i, y}| \\), prioritizing those with high correlation with \\( y \\).\n\n---\n\n#### **3. Tabular-to-Image Conversion**\n**Goal:** Map tabular data to 2D synthetic images for CNN training.  \n- **Grid Creation:**  \n  Features were distributed on a \\( \\lceil \\sqrt{k} \\rceil \\times \\lceil \\sqrt{k} \\rceil \\) grid (\\( k \\) = number of features).  \n- **Feature Placement:**  \n  For each feature \\( i \\):  \n  \\[\n  r = \\left\\lfloor \\sqrt{i} \\right\\rfloor, \\quad c = i \\% \\text{cols}\n  \\]\n  \\[\n  \\text{if } r > \\text{rows} - 1 \\implies r \\leftarrow r - 1, \\quad c \\leftarrow c + 1\n  \\]\n- **Normalization:**  \n  Images were scaled to \\( [0, 1] \\), resized to \\( 64 \\times 64 \\), and converted to RGB for CNN compatibility.\n\n---\n\n#### **4. Teacher Model (CNN)**\n**Goal:** Learn survival patterns from synthetic images using a pre-trained CNN.  \n- **Architecture:**  \n  MobileNetV2 with pre-trained weights loaded locally:  \n  \\[\n  \\text{MobileNetV2} \\rightarrow \\text{Dense Layers} \\rightarrow \\text{Survival Probability}\n  \\]\n- **Training:**  \n  Trained with \\( \\text{MSE loss} \\) on augmented images to improve robustness.\n\n---\n\n#### **5. Iterative FCNN Generations**\n**Goal:** Refine feature extraction through progressive learning.  \n- **Generations:**  \n  Trained \\( G = 3 \\) FCNNs sequentially:  \n  \\[\n  \\text{Generation}_g \\text{ learns from } \\text{Generation}_{g-1} \\text{ predictions}\n  \\]\n- **Architecture:**  \n  Each FCNN had:  \n  \\[\n  \\text{Input} \\rightarrow \\text{Dense(256)} \\rightarrow \\text{Dropout(0.5)} \\rightarrow \\text{Dense(128)} \\rightarrow \\text{Dense(64)} \\rightarrow \\text{Output}\n  \\]\n- **Loss Function:**  \n  Trained with \\( \\text{MSE loss} \\):  \n  \\[\n  \\mathcal{L}_{\\text{MSE}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n  \\]\n\n---\n\n#### **6. Distilled Feature Selection**\n**Goal:** Retain the most valuable features from iterative FCNN generations.  \n- **Cross-Validation:**  \n  Trained XGBoost on distilled features across \\( 5 \\)-folds to compute importance scores.  \n- **Selection:**  \n  Kept top \\( 20\\% \\) of features with the highest importance:  \n  \\[\n  \\text{Selected Features} = \\left\\{ f \\in \\text{Features} \\mid \\text{Importance}(f) > \\text{Threshold} \\right\\}\n  \\]\n\n---\n\n#### **7. Final XGBoost Model**\n**Goal:** Combine original features with distilled features for optimal performance.  \n- **Baseline Features:**  \n  Used raw tabular data (no imputation for numerical features).  \n- **Distilled Features:**  \n  Added predictions from all FCNN generations (filtered to top \\( 20\\% \\)).  \n- **Hyperparameters:**  \n  Original XGBoost parameters remained unchanged:  \n  \\[\n  \\text{Parameters} = \\left\\{\n    \\begin{array}{l}\n      \\text{max_depth} = 3, \\\\\n      \\text{learning_rate} = 0.02, \\\\\n      \\text{device} = \\text{cuda}, \\\\\n      \\text{enable_categorical} = \\text{True}\n    \\end{array}\n  \\right.\n  \\]\n\n---\n\n#### **8. Submission Preparation**\n**Goal:** Generate test predictions without data leakage.  \n- **Test Preprocessing:**  \n  Applied the same transformations as training (imputation, variance filtering).  \n- **Iterative FCNN Predictions:**  \n  Generated predictions for each generation and concatenated them.  \n- **Averaging:**  \n  Cross-validated predictions were averaged to reduce variance:  \n  \\[\n  \\hat{y}_{\\text{final}} = \\frac{1}{G} \\sum_{g=1}^{G} \\hat{y}_{g}\n  \\]\n- **Clipping:**  \n  Ensured predictions \\( \\in [0, 1] \\).\n\n---\n\n#### **Key Innovations**\n1. **Tabular-to-Image Conversion:**  \n   Enabled CNNs to learn spatial patterns from tabular data.  \n2. **Iterative FCNN Generations:**  \n   A knowledge distillation framework where each model learns from prior predictions:  \n   \\[\n   \\text{Generation}_g \\text{ input} = \\left[ \\text{Baseline Features}, \\hat{y}_{1}, \\hat{y}_{2}, \\dots, \\hat{y}_{g-1} \\right]\n   \\]\n3. **Hybrid Architecture:**  \n   Combined CNN’s pattern recognition with XGBoost’s robustness for survival analysis.\n\n---\n\n\n---\n\n#### **Code Structure**\n# 1. Preprocessing → Clean data for XGBoost and neural networks\n# 2. Image Generation → Create synthetic images from reordered features\n# 3. Teacher Training → Train MobileNetV2 on images\n# 4. Iterative FCNNs → Generate predictions across generations\n# 5. Feature Selection → Retain top distilled features\n# 6. Final XGBoost → Combine features and train with baseline hyperparameters\n# 7. Submission → Process test data and average predictions","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:33:54.01161Z","iopub.execute_input":"2025-04-24T13:33:54.011979Z","iopub.status.idle":"2025-04-24T13:33:54.441356Z","shell.execute_reply.started":"2025-04-24T13:33:54.011952Z","shell.execute_reply":"2025-04-24T13:33:54.440396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#lifelines\n!pip install /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n!pip install /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:33:56.132438Z","iopub.execute_input":"2025-04-24T13:33:56.132957Z","iopub.status.idle":"2025-04-24T13:34:21.859017Z","shell.execute_reply.started":"2025-04-24T13:33:56.132909Z","shell.execute_reply":"2025-04-24T13:34:21.857613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Tab to images converter since internet is off\nimport numpy as np\nfrom math import ceil, sqrt\n\nclass Tab2Img:\n    def __init__(self, save_path=None, allow_pickle=False):\n        self.save_path = save_path\n        self.allow_pickle = allow_pickle\n\n    def fit(self, X, Y):\n        n_sample, n_attrs = X.shape\n        size = ceil(sqrt(n_attrs))\n\n        # Delete constant features (std == 0) from X\n        std_X             = np.std(X, axis=0, ddof=0)\n        constant_features = np.where(std_X == 0)\n        X                 = np.delete(X, constant_features, axis=1)\n        \n        # Pearson's Corrleation Coefficient\n        # en.wikipedia.org/wiki/Pearson_correlation_coefficient\n        std_X  = np.std(X, axis=0, ddof=0)\n        std_Y  = np.std(Y, ddof=0)\n        mean_X = np.mean(X, axis=0)\n        mean_Y = np.mean(Y)\n        cov  = (X - mean_X) * (Y - mean_Y).reshape(n_sample, 1)\n        corr = np.divide(cov.sum(axis=0), std_X*std_Y*n_sample)\n\n        # The correlation vector ('corr') gets sorted in ascending mode\n        # and for each element in the sorted vector, we map the index to\n        # its place (i.e. find the row and col) in the matrix representing\n        # the sample as image. \n        indices = np.argsort(corr)[::-1]\n\n        # The map from tabular data to image data:\n        # [table column index, tensor row, tensor column]\n        self.mapping = np.zeros((len(indices), 3), np.int8)\n        \n        for i, index in enumerate(indices):\n            closest_sqrt = ceil(sqrt(i+1))\n            distance = closest_sqrt ** 2 - (i+1)\n            if distance == 0:\n                row, col = closest_sqrt, closest_sqrt\n            else:\n                if distance % 2 == 0:\n                    row, col = closest_sqrt - distance/2, closest_sqrt\n                else:\n                    row, col = closest_sqrt, closest_sqrt - ceil(distance/2)\n            self.mapping[i, :] = [index, int(row-1), int(col-1)]\n\n    def transform(self, X):\n        n_sample, n_attrs = X.shape\n        size = ceil(sqrt(n_attrs))\n        \n        # The final tensor\n        images  = np.zeros((n_sample, size, size), np.float32)\n        for index, row, col in self.mapping:\n            images[:, row, col] = X[:, index]\n\n        if self.save_path: np.save(save_path, allow_pickle=allow_pickle)\n            \n        return images\n\n    def fit_transform(self, X, Y):\n        self.fit(X, Y)\n        \n        return self.transform(X)\n\n        \n        \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:34:21.86065Z","iopub.execute_input":"2025-04-24T13:34:21.860974Z","iopub.status.idle":"2025-04-24T13:34:21.873024Z","shell.execute_reply.started":"2025-04-24T13:34:21.860947Z","shell.execute_reply":"2025-04-24T13:34:21.871848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport pandas.api.types\nimport numpy as np\nfrom lifelines.utils import concordance_index\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \"\"\"\n    >>> import pandas as pd\n    >>> row_id_column_name = \"id\"\n    >>> y_pred = {'prediction': {0: 1.0, 1: 0.0, 2: 1.0}}\n    >>> y_pred = pd.DataFrame(y_pred)\n    >>> y_pred.insert(0, row_id_column_name, range(len(y_pred)))\n    >>> y_true = { 'efs': {0: 1.0, 1: 0.0, 2: 0.0}, 'efs_time': {0: 25.1234,1: 250.1234,2: 2500.1234}, 'race_group': {0: 'race_group_1', 1: 'race_group_1', 2: 'race_group_1'}}\n    >>> y_true = pd.DataFrame(y_true)\n    >>> y_true.insert(0, row_id_column_name, range(len(y_true)))\n    >>> score(y_true.copy(), y_pred.copy(), row_id_column_name)\n    0.75\n    \"\"\"\n    \n    del solution[row_id_column_name]\n    del submission[row_id_column_name]\n    \n    event_label = 'efs'\n    interval_label = 'efs_time'\n    prediction_label = 'prediction'\n    for col in submission.columns:\n        if not pandas.api.types.is_numeric_dtype(submission[col]):\n            raise ParticipantVisibleError(f'Submission column {col} must be a number')\n    # Merging solution and submission dfs on ID\n    merged_df = pd.concat([solution, submission], axis=1)\n    merged_df.reset_index(inplace=True)\n    merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n    metric_list = []\n    for race in merged_df_race_dict.keys():\n        # Retrieving values from y_test based on index\n        indices = sorted(merged_df_race_dict[race])\n        merged_df_race = merged_df.iloc[indices]\n        # Calculate the concordance index\n        c_index_race = concordance_index(\n                        merged_df_race[interval_label],\n                        -merged_df_race[prediction_label],\n                        merged_df_race[event_label])\n        metric_list.append(c_index_race)\n    return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:34:21.875308Z","iopub.execute_input":"2025-04-24T13:34:21.87568Z","iopub.status.idle":"2025-04-24T13:34:22.591602Z","shell.execute_reply.started":"2025-04-24T13:34:21.875653Z","shell.execute_reply":"2025-04-24T13:34:22.590308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries\n\n\n\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBRegressor\nfrom lifelines import KaplanMeierFitter\n\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nimport numpy as np\nimport pandas as pd\nimport sys\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (\n    Conv2D, MaxPooling2D, Flatten, Dense, LeakyReLU, Dropout, Input\n)\nfrom tensorflow.keras.optimizers import AdamW\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nfrom metric import score  \n\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBRegressor\nfrom lifelines import KaplanMeierFitter\n\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import AdamW\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom skimage.transform import resize\nfrom metric import score\nimport tensorflow as tf\nimport xgboost as xgb\nimport numpy as np\nimport pandas as pd\n#from tab2img.converter import Tab2Img\nfrom skimage.transform import resize\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout\nfrom tensorflow.keras.optimizers import AdamW\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom metric import score\nfrom sklearn.feature_selection import VarianceThreshold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:34:22.593033Z","iopub.execute_input":"2025-04-24T13:34:22.593558Z","iopub.status.idle":"2025-04-24T13:34:38.101844Z","shell.execute_reply.started":"2025-04-24T13:34:22.593528Z","shell.execute_reply":"2025-04-24T13:34:38.100894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#the baseline XGBoost training with basic data processing\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\n\ntest = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\nprint(\"Test shape:\", test.shape )\n\ntrain = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\nprint(\"Train shape:\",train.shape)\ntrain.head()\nplt.hist(train.loc[train.efs==1,\"efs_time\"],bins=100,label=\"efs=1, Yes Event\")\nplt.hist(train.loc[train.efs==0,\"efs_time\"],bins=100,label=\"efs=0, Maybe Event\")\nplt.xlabel(\"Time of Observation, efs_time\")\nplt.ylabel(\"Density\")\nplt.title(\"Times of Observation. Either time to event, or time observed without event.\")\nplt.legend()\nplt.show()\nfrom lifelines import KaplanMeierFitter\ndef transform_survival_probability(df, time_col='efs_time', event_col='efs'):\n    kmf = KaplanMeierFitter()\n    kmf.fit(df[time_col], df[event_col])\n    y = kmf.survival_function_at_times(df[time_col]).values\n    return y\ntrain[\"y\"] = transform_survival_probability(train, time_col='efs_time', event_col='efs')\nplt.hist(train.loc[train.efs==1,\"y\"],bins=100,label=\"efs=1, Yes Event\")\nplt.hist(train.loc[train.efs==0,\"y\"],bins=100,label=\"efs=0, Maybe Event\")\nplt.xlabel(\"Transformed Target y\")\nplt.ylabel(\"Density\")\nplt.title(\"KaplanMeier Transformed Target y using both efs and efs_time.\")\nplt.legend()\nplt.show()\nRMV = [\"ID\",\"efs\",\"efs_time\",\"y\"]\nFEATURES = [c for c in train.columns if not c in RMV]\nprint(f\"There are {len(FEATURES)} FEATURES: {FEATURES}\")\nCATS = []\nfor c in FEATURES:\n    if train[c].dtype==\"object\":\n        CATS.append(c)\n        train[c] = train[c].fillna(\"NAN\")\n        test[c] = test[c].fillna(\"NAN\")\nprint(f\"In these features, there are {len(CATS)} CATEGORICAL FEATURES: {CATS}\")\ncombined = pd.concat([train,test],axis=0,ignore_index=True)\n#print(\"Combined data shape:\", combined.shape )\n# LABEL ENCODE CATEGORICAL FEATURES\nprint(\"We LABEL ENCODE the CATEGORICAL FEATURES: \",end=\"\")\nfor c in FEATURES:\n\n    # LABEL ENCODE CATEGORICAL AND CONVERT TO INT32 CATEGORY\n    if c in CATS:\n        print(f\"{c}, \",end=\"\")\n        combined[c],_ = combined[c].factorize()\n        combined[c] -= combined[c].min()\n        combined[c] = combined[c].astype(\"int32\")\n        combined[c] = combined[c].astype(\"category\")\n        \n    # REDUCE PRECISION OF NUMERICAL TO 32BIT TO SAVE MEMORY\n    else:\n        if combined[c].dtype==\"float64\":\n            combined[c] = combined[c].astype(\"float32\")\n        if combined[c].dtype==\"int64\":\n            combined[c] = combined[c].astype(\"int32\")\ntrain = combined.iloc[:len(train)].copy()\ntest = combined.iloc[len(train):].reset_index(drop=True).copy()\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBRegressor, XGBClassifier\nimport xgboost as xgb\nprint(\"Using XGBoost version\",xgb.__version__)\n\nFOLDS = 10\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\noof_xgb = np.zeros(len(train))\npred_xgb = np.zeros(len(test))\nfor i, (train_index, test_index) in enumerate(kf.split(train)):\n\n    print(\"#\"*25)\n    print(f\"### Fold {i+1}\")\n    print(\"#\"*25)\n    \n    x_train = train.loc[train_index,FEATURES].copy()\n    y_train = train.loc[train_index,\"y\"]\n    x_valid = train.loc[test_index,FEATURES].copy()\n    y_valid = train.loc[test_index,\"y\"]\n    x_test = test[FEATURES].copy()\n\n    model_xgb = XGBRegressor(\n        device=\"cuda\",\n        max_depth=3,  \n        colsample_bytree=0.5,  \n        subsample=0.8,  \n        n_estimators=2000,  \n        learning_rate=0.02,  \n        enable_categorical=True,\n        min_child_weight=80,\n        #early_stopping_rounds=25,\n    )\n    model_xgb.fit(\n        x_train, y_train,\n        eval_set=[(x_valid, y_valid)],  \n        verbose=500  \n    )\n\n    # INFER OOF\n    oof_xgb[test_index] = model_xgb.predict(x_valid)\n    # INFER TEST\n    pred_xgb += model_xgb.predict(x_test)\n# COMPUTE AVERAGE TEST PREDS\npred_xgb /= FOLDS\nfrom metric import score\ny_true = train[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\ny_pred = train[[\"ID\"]].copy()\ny_pred[\"prediction\"] = oof_xgb\nm = score(y_true.copy(), y_pred.copy(), \"ID\")\nprint(f\"\\nOverall CV for XGBoost KaplanMeier =\",m)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:34:38.102923Z","iopub.execute_input":"2025-04-24T13:34:38.103633Z","iopub.status.idle":"2025-04-24T13:36:49.425083Z","shell.execute_reply.started":"2025-04-24T13:34:38.103603Z","shell.execute_reply":"2025-04-24T13:36:49.423883Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Distiller class\nclass Distiller(tf.keras.Model):\n    def __init__(self, student, teacher):\n        super(Distiller, self).__init__()\n        self.student = student\n        self.teacher = teacher\n\n    def call(self, inputs):\n        x_images, x_tabular = inputs\n        return self.student(x_tabular)\n\n    def compile(self, optimizer, metrics, student_loss, distillation_loss, alpha):\n        super(Distiller, self).compile(\n            optimizer=optimizer,\n            loss=None,\n            metrics=metrics\n        )\n        self.student_loss = student_loss\n        self.distillation_loss = distillation_loss\n        self.alpha = alpha\n\n    def train_step(self, data):\n        x, y = data\n        x_images, x_tabular = x\n        \n        # Teacher predictions (no gradients)\n        teacher_pred = self.teacher(x_images, training=False)\n        \n        with tf.GradientTape() as tape:\n            # Student predictions\n            student_pred = self.student(x_tabular, training=True)\n            \n            # Loss computation\n            student_loss = self.student_loss(y, student_pred)\n            distillation_loss = self.distillation_loss(\n                teacher_pred, student_pred\n            )\n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n\n        # Compute gradients\n        gradients = tape.gradient(loss, self.student.trainable_variables)\n        \n        # Clip gradients to avoid NaN/Inf\n        clipped_gradients = []\n        for g in gradients:\n            # Check if gradient is finite\n            is_finite = tf.math.is_finite(g)\n            all_finite = tf.reduce_all(is_finite)\n            \n            # Clip gradients if valid, else zero them\n            g_clipped = tf.clip_by_value(g, -1e6, 1e6)\n            g_valid = tf.where(is_finite, g_clipped, tf.zeros_like(g))\n            clipped_gradients.append(g_valid)\n        \n        # Apply gradients\n        self.optimizer.apply_gradients(zip(clipped_gradients, self.student.trainable_variables))\n\n        # Update metrics\n        metrics = {}\n        for metric in self.metrics:\n            metric.update_state(y, student_pred)\n            metrics[metric.name] = metric.result()\n        return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:36:49.426139Z","iopub.execute_input":"2025-04-24T13:36:49.426517Z","iopub.status.idle":"2025-04-24T13:36:49.436218Z","shell.execute_reply.started":"2025-04-24T13:36:49.426489Z","shell.execute_reply":"2025-04-24T13:36:49.43516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define baseline parameters from the XGBoost baseline model for exact performance comparison\nbaseline_params = {\n    \"device\": \"cuda\",\n    \"max_depth\": 3,\n    \"colsample_bytree\": 0.5,\n    \"subsample\": 0.8,\n    \"n_estimators\": 2000,\n    \"learning_rate\": 0.02,\n    \"enable_categorical\": True,\n    \"min_child_weight\": 80\n}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:36:49.437127Z","iopub.execute_input":"2025-04-24T13:36:49.437442Z","iopub.status.idle":"2025-04-24T13:36:49.459675Z","shell.execute_reply.started":"2025-04-24T13:36:49.437418Z","shell.execute_reply":"2025-04-24T13:36:49.458271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Load data\ntest = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\ntrain = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\n\n#train = train.sample(frac=0.1, random_state=42).reset_index(drop=True)\n#test = test.sample(frac=0.1, random_state=42).reset_index(drop=True)\n# Add transformed target 'y'\ndef transform_survival_probability(df):\n    kmf = KaplanMeierFitter()\n    kmf.fit(df[\"efs_time\"], event_observed=df[\"efs\"])\n    y = kmf.survival_function_at_times(df[\"efs_time\"]).values\n    return y.flatten()\n\ntrain[\"y\"] = transform_survival_probability(train)\n\n# Define features and categories\nRMV = [\"ID\", \"efs\", \"efs_time\", \"y\"]\nFEATURES = [col for col in train.columns if col not in RMV]\nCATS = [col for col in FEATURES if train[col].dtype == \"object\"]\n\n# Combine train/test (baseline preprocessing)\ncombined = pd.concat([train, test], axis=0, ignore_index=True)\n\n# Process categorical features (baseline style)\nfor c in CATS:\n    combined[c] = combined[c].fillna(\"NAN\")\n    combined[c], _ = combined[c].factorize()\n    combined[c] = combined[c].astype(\"int32\").astype(\"category\")\n\n# Reduce precision for numerical features (baseline)\nnumerical_cols = [col for col in FEATURES if col not in CATS]\nfor c in numerical_cols:\n    if combined[c].dtype == \"float64\":\n        combined[c] = combined[c].astype(\"float32\")\n    elif combined[c].dtype == \"int64\":\n        combined[c] = combined[c].astype(\"int32\")\n\n# Split into train/test\ntrain_processed = combined.iloc[:len(train)].copy()\ntest_processed = combined.iloc[len(train):].reset_index(drop=True).copy()\n\nclean_train_xgb = train_processed[FEATURES].values  # Baseline (no numerical imputation)\nclean_test_xgb = test_processed[FEATURES].values\n\n# Get indices of numerical columns\nnumerical_indices = [FEATURES.index(col) for col in numerical_cols]\n\n# Impute numerical features for neural networks \nimputer = SimpleImputer(strategy='mean')\nclean_train_nn = clean_train_xgb.copy()\nclean_train_nn[:, numerical_indices] = imputer.fit_transform(\n    clean_train_xgb[:, numerical_indices]\n)\n\nclean_test_nn = clean_test_xgb.copy()\nclean_test_nn[:, numerical_indices] = imputer.transform(\n    clean_test_xgb[:, numerical_indices]\n)\n\nprint(\"Baseline features shape (no imputation):\", clean_train_xgb.shape)\nprint(\"NN features shape (imputed numerical):\", clean_train_nn.shape)\n\n# Cross-validated feature importance for Tab2Img\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nimportances = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train_processed)):\n    x_tr = clean_train_xgb[train_idx]\n    y_tr = train[\"y\"].iloc[train_idx]\n    \n    xgb_model = XGBRegressor()\n    xgb_model.fit(x_tr, y_tr)\n    importance = xgb_model.feature_importances_\n    importances.append(importance)\n\nimportance = np.mean(importances, axis=0)\nimportance_normalized = importance / np.max(importance)\nsorted_indices = np.argsort(importance)[::-1]\n\n# Remove low-variance features\nselector = VarianceThreshold(threshold=0.01)\nclean_train_filtered = selector.fit_transform(clean_train_xgb)\nclean_test_filtered = selector.transform(clean_test_xgb)\n\n# Update FEATURES list\nselected_features = selector.get_support(indices=True)\nfiltered_FEATURES = [FEATURES[i] for i in selected_features]\n\n# Re-compute importance on filtered features\nkf = KFold(n_splits=5)\nimportances_filtered = []\n\nfor fold, (tr_idx, val_idx) in enumerate(kf.split(train_processed)):\n    model = XGBRegressor(**baseline_params)\n    model.fit(clean_train_filtered[tr_idx], train[\"y\"].iloc[tr_idx])\n    importances_filtered.append(model.feature_importances_)\n\nimportance_filtered = np.mean(importances_filtered, axis=0)\nsorted_indices_filtered = np.argsort(importance_filtered)[::-1]\n\nclean_train_weighted = clean_train_nn[:, sorted_indices] * importance_normalized[sorted_indices]\nclean_test_weighted = clean_test_nn[:, sorted_indices] * importance_normalized[sorted_indices]\n\n# Generate grayscale images\nimage_convertor = Tab2Img()\nX_train_images = image_convertor.fit_transform(clean_train_weighted, train[\"y\"].values)\nX_test_images = image_convertor.transform(clean_test_weighted)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:36:49.46282Z","iopub.execute_input":"2025-04-24T13:36:49.463214Z","iopub.status.idle":"2025-04-24T13:37:09.924026Z","shell.execute_reply.started":"2025-04-24T13:36:49.463172Z","shell.execute_reply":"2025-04-24T13:37:09.9231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute Pearson correlation between features and target\ndef compute_correlation(X, y):\n    correlations = []\n    for i in range(X.shape[1]):\n        corr = np.corrcoef(X[:, i], y)[0, 1]\n        correlations.append(corr)\n    return correlations\n\n# Sort features by absolute correlation\nimportances = compute_correlation(clean_train_xgb, train[\"y\"])\nsorted_indices = np.argsort(np.abs(importances))[::-1]\n\n# Reorder features\nclean_train_reordered = clean_train_xgb[:, sorted_indices]\nclean_test_reordered = clean_test_xgb[:, sorted_indices]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:37:09.92538Z","iopub.execute_input":"2025-04-24T13:37:09.925772Z","iopub.status.idle":"2025-04-24T13:37:09.962577Z","shell.execute_reply.started":"2025-04-24T13:37:09.925745Z","shell.execute_reply":"2025-04-24T13:37:09.961637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tab2img(x, k):\n    sqrt_k = np.sqrt(k)\n    rows = cols = int(np.ceil(sqrt_k))\n    \n    image = np.zeros((rows, cols))\n    \n    for idx in range(k):\n        r = int(np.sqrt(idx))\n        c = idx % cols\n        \n        if r > rows - 1:\n            r -= 1\n            c += 1\n        \n        image[r, c] = x[idx]\n    \n    return image\n\n# Generate images\nX_train_images = np.array([tab2img(sample, len(FEATURES)) for sample in clean_train_reordered])\nX_test_images = np.array([tab2img(sample, len(FEATURES)) for sample in clean_test_reordered])\n\nprint(\"Image shape:\", X_train_images.shape)  # Should be (n_samples, rows, cols)\n\n# Preprocess images (resize to 64x64 RGB)\ndef preprocess_images(images, up=64):\n    images_2d = images.reshape(images.shape[0], -1)\n    imputer = SimpleImputer(strategy='mean')\n    images_imputed = imputer.fit_transform(images_2d)\n    images_imputed = images_imputed.reshape(images.shape)\n    images_normalized = images_imputed / 255.0\n    resized_images = np.array([resize(img, (up, up), anti_aliasing=True) for img in images_normalized])\n    rgb_images = np.repeat(resized_images[..., np.newaxis], 3, axis=-1)\n    return rgb_images\n\nX_train_images_rgb = preprocess_images(X_train_images)\nX_test_images_rgb = preprocess_images(X_test_images)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:37:09.963674Z","iopub.execute_input":"2025-04-24T13:37:09.964096Z","iopub.status.idle":"2025-04-24T13:37:24.171094Z","shell.execute_reply.started":"2025-04-24T13:37:09.964052Z","shell.execute_reply":"2025-04-24T13:37:24.170133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocess images (resize to 64x64 RGB)\ndef preprocess_images(images, up=64):\n    images_2d = images.reshape(images.shape[0], -1)\n    imputer = SimpleImputer(strategy='mean')\n    images_imputed = imputer.fit_transform(images_2d)\n    images_imputed = images_imputed.reshape(images.shape)\n    images_normalized = images_imputed / 255.0\n    resized_images = np.array([resize(img, (up, up), anti_aliasing=True) for img in images_normalized])\n    rgb_images = np.repeat(resized_images[..., np.newaxis], 3, axis=-1)\n    return rgb_images\n\nX_train_images_rgb = preprocess_images(X_train_images)\nX_test_images_rgb = preprocess_images(X_test_images)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:37:24.172085Z","iopub.execute_input":"2025-04-24T13:37:24.172453Z","iopub.status.idle":"2025-04-24T13:37:35.045174Z","shell.execute_reply.started":"2025-04-24T13:37:24.17242Z","shell.execute_reply":"2025-04-24T13:37:35.044124Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef visualize_images_and_labels(images, labels, n_samples=5):\n    fig, axes = plt.subplots(1, n_samples, figsize=(20, 4))\n    for i in range(n_samples):\n        ax = axes[i]\n        img_idx = np.random.choice(len(images))\n        ax.imshow(images[img_idx])\n        ax.set_title(f\"Label y = {labels.iloc[img_idx]:.2f}\")\n        ax.axis('off')\n    plt.show()\n\nvisualize_images_and_labels(X_train_images_rgb, train[\"y\"])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:37:35.046279Z","iopub.execute_input":"2025-04-24T13:37:35.046552Z","iopub.status.idle":"2025-04-24T13:38:54.377207Z","shell.execute_reply.started":"2025-04-24T13:37:35.046531Z","shell.execute_reply":"2025-04-24T13:38:54.375279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load weights from local path\ndef create_teacher():\n    base_model = MobileNetV2(\n        input_shape=(64, 64, 3),\n        include_top=False,\n        weights='/kaggle/input/mobilenetv2/keras/default/1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5',\n        pooling='avg'\n    )\n    base_model.trainable = False\n\n    model = Sequential([\n        base_model,\n        Dense(128, activation='relu'),\n        Dropout(0.3),\n        Dense(1, activation='linear')\n    ])\n    \n    return model\nteacher = create_teacher()\nteacher.compile(\n    optimizer=AdamW(\n        learning_rate=1e-4,\n        weight_decay=1e-5\n    ),\n    loss='mse',\n    metrics=['mse']\n)\n\n# Train teacher with early stopping\nteacher.fit(\n    X_train_images_rgb,\n    train[\"y\"],\n    batch_size=16,\n    validation_split=0.2,\n    epochs=30,\n    callbacks=[EarlyStopping(\n        monitor='val_loss',\n        patience=5,\n        restore_best_weights=True\n    )],\n    verbose=1\n)\n\n# Student model (trained from scratch)\ndef create_student(input_dim):\n    model = Sequential([\n        Dense(256, activation='relu', input_dim=input_dim, kernel_regularizer='l2'),\n        Dropout(0.5),\n        Dense(128, activation='relu', kernel_regularizer='l2'),\n        Dense(64, activation='relu', kernel_regularizer='l2'),\n        Dense(1, activation='linear')\n    ])\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:38:54.378914Z","iopub.execute_input":"2025-04-24T13:38:54.379318Z","iopub.status.idle":"2025-04-24T13:47:50.273276Z","shell.execute_reply.started":"2025-04-24T13:38:54.379283Z","shell.execute_reply":"2025-04-24T13:47:50.271859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Initialize iterative FCNN generations\ngenerations = 3\ncurrent_train = clean_train_nn.copy()  # Start with imputed tabular features\ncurrent_test = clean_test_nn.copy()\n\nall_distilled_features_train = []\nall_distilled_features_test = []\n\nfor gen in range(generations):\n    print(f\"Training FCNN Generation {gen+1}\")\n    \n    def create_fcnn(input_dim):\n        model = Sequential([\n            Dense(256, activation='relu', input_dim=input_dim, kernel_regularizer='l2'),\n            Dropout(0.5),\n            Dense(128, activation='relu', kernel_regularizer='l2'),\n            Dense(64, activation='relu', kernel_regularizer='l2'),\n            Dense(1, activation='linear')\n        ])\n        \n        return model\n    \n    fcnn = create_fcnn(input_dim=current_train.shape[1])\n    fcnn.compile(\n        optimizer=AdamW(learning_rate=1e-4),\n        loss='mse'\n    )\n    \n    fcnn.fit(\n        current_train,\n        train[\"y\"],\n        batch_size=16,\n        validation_split=0.2,\n        epochs=30,\n        callbacks=[EarlyStopping(monitor='val_loss', patience=5)],\n        verbose=0\n    )\n    \n    # Get predictions for train and test\n    fcnn_pred_train = fcnn.predict(current_train).flatten()\n    fcnn_pred_test = fcnn.predict(current_test).flatten()\n    \n    # Append predictions to current features\n    current_train = np.concatenate(\n        [current_train, fcnn_pred_train.reshape(-1, 1)],\n        axis=1\n    )\n    \n    current_test = np.concatenate(\n        [current_test, fcnn_pred_test.reshape(-1, 1)],\n        axis=1\n    )\n    \n    # Store predictions\n    all_distilled_features_train.append(fcnn_pred_train)\n    all_distilled_features_test.append(fcnn_pred_test)\n\n# Combine all distilled predictions\ndistilled_features_train = np.column_stack(all_distilled_features_train)\ndistilled_features_test = np.column_stack(all_distilled_features_test)\n\n# Cross-validate distilled features (same as before)\nkf = KFold(n_splits=5)\ndistilled_importances = []\n\nfor fold, (tr_idx, val_idx) in enumerate(kf.split(train_processed)):\n    model = XGBRegressor()\n    model.fit(\n        distilled_features_train[tr_idx],\n        train[\"y\"].iloc[tr_idx]\n    )\n    importance = model.feature_importances_\n    distilled_importances.append(importance)\n\ndistilled_importance = np.mean(distilled_importances, axis=0)\nselected_features = np.where(\n    distilled_importance > np.percentile(distilled_importance, 80)\n)[0]\n\ndistilled_features_train_selected = distilled_features_train[:, selected_features]\ndistilled_features_test_selected = distilled_features_test[:, selected_features]\n\n# Combine with baseline features\nX_train_final = np.concatenate(\n    [clean_train_xgb, distilled_features_train_selected],\n    axis=1\n)\n\nX_test_final = np.concatenate(\n    [clean_test_xgb, distilled_features_test_selected],\n    axis=1\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:47:50.27465Z","iopub.execute_input":"2025-04-24T13:47:50.275058Z","iopub.status.idle":"2025-04-24T13:52:29.038724Z","shell.execute_reply.started":"2025-04-24T13:47:50.275009Z","shell.execute_reply":"2025-04-24T13:52:29.037788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train final Distilled XGBoost (baseline parameters)\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\noof_distilled = np.zeros(len(train))\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train_processed)):\n    x_tr = X_train_final[train_idx]\n    y_tr = train[\"y\"].iloc[train_idx]\n    x_val = X_train_final[val_idx]\n    y_val = train[\"y\"].iloc[val_idx]\n    \n    model = XGBRegressor(\n        device=\"cuda\",\n        max_depth=3,\n        colsample_bytree=0.5,\n        subsample=0.8,\n        n_estimators=2000,\n        learning_rate=0.02,\n        enable_categorical=True,\n        min_child_weight=80\n    )\n    \n    model.fit(\n        x_tr, y_tr,\n        eval_set=[(x_val, y_val)],\n        early_stopping_rounds=25,\n        verbose=100\n    )\n    \n    oof_distilled[val_idx] = model.predict(x_val)\n\n# Compute competition score\ny_pred_distilled = train[[\"ID\"]].assign(prediction=oof_distilled)\ndistilled_score = score(\n    train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]],\n    y_pred_distilled,\n    \"ID\"\n)\nprint(f\"Final Score: {distilled_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:52:29.03961Z","iopub.execute_input":"2025-04-24T13:52:29.040343Z","iopub.status.idle":"2025-04-24T13:53:43.319469Z","shell.execute_reply.started":"2025-04-24T13:52:29.040312Z","shell.execute_reply":"2025-04-24T13:53:43.318231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load test data\ntest_processed = combined.iloc[len(train):].reset_index(drop=True).copy()\nclean_test_xgb = test_processed[FEATURES].values  # Baseline (no imputation)\nclean_test_nn = clean_test_xgb.copy()\n\n# Impute numerical features for neural networks (same as training)\nnumerical_indices = [FEATURES.index(col) for col in numerical_cols]\nclean_test_nn[:, numerical_indices] = imputer.transform(\n    clean_test_xgb[:, numerical_indices]\n)\n\n# Generate images for test set\nclean_test_weighted = clean_test_nn * importance_normalized\nX_test_images = image_convertor.transform(clean_test_weighted)\nX_test_images_rgb = preprocess_images(X_test_images)\n\n# Extract distilled features from test set\ndistilled_features_test = np.column_stack(all_distilled_features_test)  # Already computed\ndistilled_features_test_selected = distilled_features_test[:, selected_features]\n\n# Combine with baseline features\nX_test_final = np.concatenate(\n    [clean_test_xgb, distilled_features_test_selected],\n    axis=1\n)\n# Initialize test predictions\ntest_preds = np.zeros(len(test))\n\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train_processed)):\n    x_tr = X_train_final[train_idx]\n    y_tr = train[\"y\"].iloc[train_idx]\n    x_val = X_train_final[val_idx]\n    \n    model = XGBRegressor(**baseline_params)\n    model.fit(\n        x_tr, y_tr,\n        eval_set=[(x_val, train[\"y\"].iloc[val_idx])],\n        early_stopping_rounds=25,\n        verbose=0\n    )\n    \n    # Predict on test set\n    fold_pred = model.predict(X_test_final)\n    test_preds += fold_pred / kf.n_splits  # Average predictions\n\n# Clip predictions to valid range \ntest_preds = np.clip(test_preds, 0, 1)\n\n# Create submission DataFrame\nsubmission = pd.DataFrame({\n    \"ID\": test[\"ID\"],\n    \"prediction\": test_preds\n})\n\n# Verify predictions are numeric\nif not submission[\"prediction\"].dtype == np.float32:\n    submission[\"prediction\"] = submission[\"prediction\"].astype(np.float32)\n\n# Save submission\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file saved successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:53:43.321016Z","iopub.execute_input":"2025-04-24T13:53:43.321386Z","iopub.status.idle":"2025-04-24T13:54:54.971616Z","shell.execute_reply.started":"2025-04-24T13:53:43.321357Z","shell.execute_reply":"2025-04-24T13:54:54.970671Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"####In conclusion, I was able to achieve comparable results to the original research paper's (Private score 0.68060) with slight improvement on the baseline XGBoost. Granted, a lot of the comparability is due to XGBoost's own structure, nevertheless the applicability and feature engineering boost that the methods described in the research paper cannot be denied. Especially considering how the original research papers successful improvements on the baselines all had wayy larger datasets in size. Yet, even with these results, I believe the truly positive part of the experience was the learning of application of knowledge distillation, statistical analysis and unbiased research analysis. ","metadata":{}}]}